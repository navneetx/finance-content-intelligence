name: Daily Finance Content Scraper

on:
  schedule:
    # Runs daily at 2:00 AM IST (8:30 PM UTC previous day)
    - cron: '30 20 * * *'
  
  # Allow manual trigger from GitHub UI
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install google-api-python-client
      
      - name: Run scraper
        env:
          YOUTUBE_API_KEY: ${{ secrets.YOUTUBE_API_KEY }}
        run: |
          # Create temporary env file for scraper
          echo "API_KEY='$YOUTUBE_API_KEY'" > temp_env.txt
          # Run scraper from root directory
          python scraper.py
          # Verify output was created
          if [ ! -f finance_content_data_*.json ]; then
            echo "Error: Scraper did not generate output file"
            exit 1
          fi
      
      - name: Find latest data file
        id: find_file
        run: |
          LATEST_FILE=$(ls -t finance_content_data_*.json 2>/dev/null | head -1)
          if [ -z "$LATEST_FILE" ]; then
            echo "Error: No data file found"
            exit 1
          fi
          echo "latest_file=$LATEST_FILE" >> $GITHUB_OUTPUT
          echo "Found file: $LATEST_FILE"
      
      - name: Copy data to dashboard
        run: |
          SOURCE_FILE="${{ steps.find_file.outputs.latest_file }}"
          DEST_FILE="dashboard/src/data/finance_content_data.json"
          
          if [ -f "$SOURCE_FILE" ]; then
            cp "$SOURCE_FILE" "$DEST_FILE"
            echo "Copied $SOURCE_FILE to $DEST_FILE"
            ls -lh "$DEST_FILE"
          else
            echo "Error: Source file not found: $SOURCE_FILE"
            exit 1
          fi
      
      - name: Configure Git
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
      
      - name: Commit and push changes
        run: |
          # Check if there are actual changes
          if git diff --quiet dashboard/src/data/finance_content_data.json; then
            echo "No changes in data file - skipping commit"
          else
            git add dashboard/src/data/finance_content_data.json
            
            # Get statistics for commit message
            VIDEO_COUNT=$(python -c "import json; data=json.load(open('dashboard/src/data/finance_content_data.json')); print(len(data))")
            
            git commit -m "ðŸ¤– Automated daily scrape - Updated with $VIDEO_COUNT videos"
            git push
            echo "âœ“ Successfully committed and pushed updates"
          fi
      
      - name: Scrape summary
        if: always()
        run: |
          echo "=== Scraper Execution Summary ==="
          echo "Timestamp: $(date)"
          echo "File size: $(du -h dashboard/src/data/finance_content_data.json | cut -f1)"
          echo "Status: ${{ job.status }}"